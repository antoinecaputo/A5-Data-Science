{
 "cells": [
  {
   "attachments": {
    "logo_ecole_ing.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "BkcSXRZGv7ss"
   },
   "source": [
    "# <center>Transfer Learning<br /> Workshop</center>\n",
    "\n",
    "![logo_ecole_ing.jpg](attachment:logo_ecole_ing.jpg)\n",
    "Concepteur : Mellila BOUAM\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdTHdaRqwSqP"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NcCLmHMge1A"
   },
   "source": [
    "L'objectif de ce workshop est de vous initier au [transfer learning](https://fr.wikipedia.org/wiki/Apprentissage_par_transfert), une technique utilisée pour exploiter les connaissances acquises par un modèle entraîné sur un grand dataset sur une tâche applicative où la quantité de données disponibles est limitée.\n",
    "\n",
    "Nous commencerons par faire une analyse intuitive de l'efficacité du transfer learning. Nous préparerons ensuite la première partie, c'est-à-dire les données qui seront utilisées, du workshop d'[annotations d'images](https://fr.wikipedia.org/wiki/Annotation_automatique_d%27images) (image captioning) dans lequel nous allons effectuer l'entraînement d'un modèle en recourant au transfer learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BP3uDiD5jfxE"
   },
   "source": [
    "## Qu'est-ce que le transfer learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqk9cBGGwXH-"
   },
   "source": [
    "Le **Transfer Learning** consiste à prendre des features apprises sur un problème, et les utiliser sur un autre problème similaire. Par exemple, les features d'un modèle qui a appris à reconnaître des chats peut être utile pour initialiser un modèle qui va reconnaître des tigres.\n",
    "\n",
    "Le transfer learning est souvent effectué sur des tâches où le dataset contient trop peu d'exemples pour entraîner un modèle à partir de zéro.\n",
    "\n",
    "Le workflow typique du transfer learning dans le contexte du deep learning est le suivant :\n",
    "\n",
    "1. Prendre les couches d'un modèle pré-entraîné.\n",
    "2. Geler les poids de ces couches.\n",
    "3. Ajouter des couches entraînables (dont les poids ne sont pas gelés) à la fin du modèle. Elle vont servir à convertir les features en prédictions sur un nouveau dataset.\n",
    "4. Entraîner les nouvelles couches sur le nouveau dataset.\n",
    "\n",
    "- Quel est l'intérêt de geler les poids des couches du modèle pré-entraîné (étape 2)? \n",
    "<em>À COMPLÉTER</em>\n",
    "\n",
    "\n",
    "Une dernière étape optionnelle, le **fine-tuning** consiste à dégeler toutes les couches du modèle obtenu après le processus précédent et le réentraîner sur le nouveau dataset avec un learning rate très bas. \n",
    "\n",
    "- Quel est l'intérêt de l'étape de fine-tuning ? Pourquoi utilise-on un learning rate très bas ?\n",
    "<em>À COMPLÉTER</em>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k2x-90wjpLd"
   },
   "source": [
    "# Partie 1 : Intuition sur le transfer learning\n",
    "\n",
    "Dans cette partie, nous allons charger le modèle [InceptionV3](https://paperswithcode.com/method/inception-v3) pré-entraîné sur une tâche de classification sur le dataset [ImageNet](https://image-net.org/). Nous allons ensuite utiliser ce modèle pour calculer des représentations vectorielles (appelées [embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?hl=fr)) d'images du dataset [cats vs dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs). Nous allons ensuite représenter chaque embedding sur le plan et interpréter les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkkl3Qk-z6Ql"
   },
   "source": [
    "## Chargement des données\n",
    "Nous chargeons premièrement le dataset cats vs. dogs en utilisant [TFDS](https://www.tensorflow.org/datasets?hl=fr) (TensorFlow Datasets).\n",
    "\n",
    "Le transfer learning marche particulièrement sur les petits datasets. Complétez le code ci-dessous, vous devez afficher le nombre d'exemples de chaque dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rPvwc-Fbs5fi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BlP4SbbtXcJ",
    "outputId": "f76edb9e-10dd-4b08-8cde-285e73947fb2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# On charge un dataset de tensorflow\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfds\u001B[39;00m\n\u001B[1;32m      5\u001B[0m tfds\u001B[38;5;241m.\u001B[39mdisable_progress_bar()\n\u001B[1;32m      7\u001B[0m train_ds, validation_ds, test_ds \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mload(\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m#A COMPLETER\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#\"cifar10\",\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     as_supervised\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# Include labels\u001B[39;00m\n\u001B[1;32m     13\u001B[0m )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "# On charge un dataset de tensorflow\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    #\"cifar10\",\n",
    "    # Reserve 10% for validation and 10% for test\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "# Nb of training samples\n",
    "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
    "# Nb of validation samples\n",
    "print(\n",
    "    #A COMPLETER\n",
    ")\n",
    "# Nb of test samples\n",
    "print(\n",
    "    #A COMPLETER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cjQ28eM0giY"
   },
   "source": [
    "Visualisons les 9 premières images du dataset. On remarquera que la classe 0 correspond à \"cat\" et la classe 1 correspond à dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "ZMchOUtPtbDR",
    "outputId": "7507eeee-f85b-46fa-ea6f-690ff06c381d"
   },
   "outputs": [],
   "source": [
    "# On affiche quelques exemples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTUBZnW10y0S"
   },
   "source": [
    "## Standardisation des données\n",
    "Les images du dataset sont de tailles différentes. Nous devons donc les redimensionner pour correspondre à la taille attendue par le modèle que l'on va utiliser : InceptionV3.\n",
    "\n",
    "Nous redimensionnons les images pour une taille de (299x299). Pour le faire, vous aurez besoin d'une fonction de redimensionnement d'images, que [TensorFLow vous fournira](https://www.tensorflow.org/api_docs/python/tf/image/resize). Par ailleurs, si vous voulez effectuer le redimensionnement en une seule instruction, vous pouvez utiliser les [expressions lambda](https://www.w3schools.com/python/python_lambda.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9n8womKqth6O"
   },
   "outputs": [],
   "source": [
    "# On resize les images \n",
    "\n",
    "size = (299, 299)\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    lambda: x, y: (tf.image.resize(x, size), y)\n",
    "    #A COMPLETER\n",
    ")\n",
    "validation_ds =  #A COMPLETER\n",
    "test_ds =  #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EOaE1ukFitIW"
   },
   "outputs": [],
   "source": [
    " # On preprocess les images pour pouvoir les donner en entrées à InceptionV3\n",
    "\n",
    "train_ds =  #A COMPLETER\n",
    "validation_ds =  #A COMPLETER\n",
    "test_ds =  #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nEeMKN6gtkNO"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds =  #A COMPLETER\n",
    "test_ds =  #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4up2BQqvQOSM",
    "outputId": "14d8be5f-b3ae-40f2-d896-e43c2f962548"
   },
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "\n",
    "# Si on met weights=None -> on aura des embeddings qui ne contiennent pas d'info\n",
    "# Si on met weights='imagenet' -> on aura des embeddings sémantiquement pertinents même si le modèle n'a été entraîné ni sur cats vs dogs ni cifar10\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet', pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TszkaGXMYJsV"
   },
   "source": [
    "À quoi correspond le paramètre **include_top = false** ?\n",
    "<em>À COMPLÉTER</em>\n",
    " \n",
    "À quoi correspond le paramètre **weights='imagenet'** ? \n",
    "<em>À COMPLÉTER</em>\n",
    "\n",
    "À quoi correspond le paramètre **pooling='avg'** ?\n",
    "<em>À COMPLÉTER</em>\n",
    "\n",
    "\n",
    "On définit maintenant le [modèle que l'on va utiliser](https://www.tensorflow.org/api_docs/python/tf/keras/Model) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBPpxW_thpYp",
    "outputId": "24f60590-758a-4af5-9f70-b987ba80facf"
   },
   "outputs": [],
   "source": [
    "# Définition du modèle d'embedding utilisant InceptionV3\n",
    "inputs = keras.Input(shape=(299, 299, 3))\n",
    "x = image_model(inputs)\n",
    "model =  #A COMPLETER\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHz2xVqnNfrC"
   },
   "source": [
    "On calcule les embeddings des images du dataset de test. Le dataset comprend 2326 images et la taille d'un embedding est de 2048 (regardez la documentation de keras.Model au dessus) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VasAv00whwc0",
    "outputId": "51d14d9e-f210-4b84-d660-bad0c6704970"
   },
   "outputs": [],
   "source": [
    "# Calcul des embeddings des images du dataset de test\n",
    "\n",
    "output = model.predict(test_ds, verbose=1) #A COMPLETER\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1DvylqDVvWf-"
   },
   "outputs": [],
   "source": [
    "# On construit la liste des vraies classes des exemples du dataset de test (1 -> chien, 0 -> chat)\n",
    "\n",
    "labels = []\n",
    "for _, y in test_ds:\n",
    "    labels.extend(\n",
    "        y.numpy().tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CALowNlYgNa"
   },
   "source": [
    "Afin de montrer la pertinence d'utiliser un modèle InceptionV3 préentraîné sur ImageNet, nous allons visualiser les embeddings du dataset de test et voir s'il y a une certaine séparation entre les classes du dataset.\n",
    "\n",
    "Afin de visualiser l'embedding d'une image, celui-ci doit être de dimension 2. Or, les embeddings calculés sont de taille 2048. Nous allons donc utiliser l'algorithme [t-SNE](https://fr.wikipedia.org/wiki/Algorithme_t-SNE) afin de réduire la dimension de 2048 à 2.\n",
    "\n",
    "t-SNE est un algorithme de réduction de dimension utilisé pour visualiser des données à grande dimension. L'algorithme tente de trouver une configuration qui préserve le plus possible la proximité entre les points lors du passage de l'espace de grande dimension (ici 2048) à l'espace de faible dimension (ici 2). Nous utiliserons l'implémentation de la library [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlhQ0fe3u42v",
    "outputId": "1fdbe255-ef49-49cc-eff7-6455b9ad1531"
   },
   "outputs": [],
   "source": [
    "# On applique l'algorithme tSNE aux embeddings afin de réduire leur dimension\n",
    "# et pouvoir les afficher (embeddings de tailles 2048 -> 2)\n",
    "from sklearn.manifold import TSNE\n",
    "tsne =  TSNE(n_components=2, verbose=1).fit_transform(output)#A COMPLETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "waCE7Iqtu_-F",
    "outputId": "755f5336-c8d6-41fb-b969-a71aec8efb13"
   },
   "outputs": [],
   "source": [
    "# On définit la couleur associée à chaque classe (seules 2 classes sont utilisées dans le cas cats vs dogs)\n",
    "\n",
    "colors = {0:'red', \n",
    "          1:'blue'}\n",
    "\n",
    "# On plot les embeddings des images calculés par InceptionV3\n",
    "# Chaque point correspond à une image et ses coordonnées sont l'embedding tsne de cette image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(*tsne.T, s=1.5, c=[colors[l] for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpqbHwUYGJL8"
   },
   "source": [
    "# Partie 2 : Préparation des données pour le transfer learning sur une tâche d'image captioning\n",
    "\n",
    "L'objectif de cette partie est de préparer les données, à travers des prétraitements, pour pouvoir entraîner un modèle sur la tâche d'image captioning. La tâche d'image captioning consiste à prendre une image en entrée et de donner en sortie, une \"phrase\" décrivant ce que contient cette image. Cette dernière partie sera abordée dans le Workshop suivant.\n",
    "\n",
    "Nous allons donc ici préparer les données pour ce prochain workshop où on entraînera un modèle composé d'un CNN InceptionV3 préentraîné sur ImageNet pour encoder les images du dataset COCO en embeddings, suivi d'un RNN qui servira à décoder l'embedding et sortir la description textuelle de l'image. Cet entraînement représente un exemple typique de transfer learning.\n",
    "\n",
    "Une légère modification que l'on peut faire au workflow de transfer learning décrit plus tôt est qu'au lieu de geler les poids du modèle préentraîné (InceptionV3), on peut précalculer les embeddings des images du dataset et les enregistrer dans le disque. Ceci servira à sacrifier un peu d'espace disque contre du temps d'éxecution (inutile de calculer les embeddings des images à chaque epochs) et de la mémoire GPU (inutile de charger le modèle InceptionV3 pendant l'entraînement).\n",
    "\n",
    "Avec cette modification par contre, il ne sera plus possible d'effectuer la phase de fine-tuning qui consiste à compléter l'entraînement du modèle InceptionV3 avec un ajustement très léger des poids (learning rate très bas) pour optimiser les embeddings qu'il construit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb5BaIf-tvtK"
   },
   "source": [
    "# 1 Téléchargement des données :\n",
    "Tout d'abord, nous devons importer les bibliothèques dont nous aurons besoin pour cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TH7tgN9wtvtM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GovJnRQHtvtO"
   },
   "source": [
    "Vous devez télécharger le dataset [MS COCO](https://cocodataset.org/#home). Ce dataset contient 82000 images annotés chacunes avec 5 légendes. Téléchargez le [dataset](http://images.cocodataset.org/zips/train2014.zip), les [annotations correspondantes](http://images.cocodataset.org/annotations/annotations_trainval2014.zip), et décompressez-les dans le bon répertoire. Le code ci-dessous affiche le chemin d'accès du dossier devant contenir les données ainsi que le fichier des annotations (`annotation_file` et `PATH`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpXL9OgstvtP",
    "outputId": "d4db57a3-27c3-4310-dcce-b6f2ce051206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation file  /Users/antoine/Library/CloudStorage/OneDrive-AssociationCesiViacesimail/Cesi/A5/Option 1 - Data Science/6 - Transfert Learning/annotations/captions_train2014.json\n",
      "le chemin PATH  /Users/antoine/Library/CloudStorage/OneDrive-AssociationCesiViacesimail/Cesi/A5/Option 1 - Data Science/6 - Transfert Learning/train2014/\n"
     ]
    }
   ],
   "source": [
    "# Chemin du fichier d'annotations\n",
    "annotation_folder = os.path.abspath('.')+\"/annotations/\"\n",
    "annotation_file = annotation_folder+\"captions_train2014.json\"\n",
    "\n",
    "# Chemin du dossier contenant les images à annoter\n",
    "image_folder = '/train2014/'\n",
    "PATH = os.path.abspath('.') + image_folder\n",
    "\n",
    "print(\"annotation file \", annotation_file)\n",
    "print(\"le chemin PATH \", PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w5nfrgdNJ061"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/antoine/Library/CloudStorage/OneDrive-AssociationCesiViacesimail/Cesi/A5/Option 1 - Data Science/6 - Transfert Learning/annotations//captions_val2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mannotation_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/captions_val2014.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      2\u001B[0m     annotations \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/antoine/Library/CloudStorage/OneDrive-AssociationCesiViacesimail/Cesi/A5/Option 1 - Data Science/6 - Transfert Learning/annotations//captions_val2014.json'"
     ]
    }
   ],
   "source": [
    "with open(annotation_folder+\"/captions_val2014.json\", 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jbg5-3cEJa0t"
   },
   "outputs": [],
   "source": [
    "with open(annotation_folder+\"instances_val2014.json\", 'r') as f:\n",
    "    instances = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zihN_KDTtvtW"
   },
   "source": [
    "On veut sélectionner un sous-ensemble d'images afin de rendre l'apprentissage plus rapide. Le but est de prendre seulement 35000 images au lieu des 82000 du dataset d'origine. Pour faire ceci, on pourra créer une structure qui associe chaque à image sa liste des annotations. Comment on pourrait représenter ces données intermédiaires ?\n",
    "<em>À COMPLÉTER</em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4BdFKu1qtvtX"
   },
   "outputs": [],
   "source": [
    "# Lecture du fichier d'annotation\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Grouper toutes les annotations ayant le meme identifiant.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "    # marquer le debut et la fin de chaque annotation\n",
    "    caption =  #A COMPLETER\n",
    "    # L'identifiant d'une image fait partie de son chemin d'accès\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    # Rajout du caption associé à image_path\n",
    "    #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ndNZCLBtvta",
    "outputId": "fe09dd77-19b2-4d24-ca3e-d9d36d0f71c7"
   },
   "outputs": [],
   "source": [
    "# Prendre les premières images seulement\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:2000]\n",
    "print(len(train_image_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcI0zgKrtvtp"
   },
   "source": [
    "Maintenant, il est temps de construire notre jeu de données à proprement parler. Le code produisant ce jeu de données est composée de :\n",
    "<ul>\n",
    "        <li>La liste `img_name_vector` qui contient les noms des fichers d'images associés à aux annotations. C'est les exemples dans le jeu de données. </li>\n",
    "        <li>La liste `train_captions` contenant les annotations. C'est les labels.</li>\n",
    "</ul>\n",
    "Vous pouvez afficher un exemple pour vérification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8AE3wje7tvtr"
   },
   "outputs": [],
   "source": [
    "# Liste de toutes les annotations\n",
    "train_captions = []\n",
    "# Liste de tous les noms de fichiers des images dupliquées (en nombre d'annotations par image)\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    # Rajout de caption_list dans train_captions\n",
    "    #A COMPLETER\n",
    "    # Rajout de image_path dupliquée len(caption_list) fois\n",
    "    #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "3ZpYOsILtvtu",
    "outputId": "2a6d5f1d-f053-4896-fc46-cd77f4f3db5a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(train_captions), len(img_name_vector))\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA1-72wntvtw"
   },
   "source": [
    "# 2 Prétraitement des images\n",
    "Ce que l'on va faire ensuite, c'est charger le réseau de neurones qui s'occupera de la partie vision du problème d’annotation. En effet, l'architecture globale du système contiendra deux parties :\n",
    "<ul>\n",
    "    <li>Un CNN dont le rôle sera de résumer et d'extraire le contenu brute et volumineux de l'information portée par les pixels en une représentation compacte qui regroupe les caractéristiques essentielles pour effectuer l'annotation.</li>\n",
    "    <li>Et un RNN qui aura en entrée cette représentation compacte et dont le but est d'apprendre le prochain mot à partir des premiers mot de l'annotation.</li>\n",
    "</ul>\n",
    "\n",
    "Pour créer un nouveau modèle vous pourrez utiliser la fonction `tf.keras.Model` de [tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/Model). Pour la partie CNN, vous n'aurez pas à concevoir votre propre architecture pour le réseau de neurones, vous devez utiliser une architecture qui existe déjà et qui se nomme [InceptionV3](https://paperswithcode.com/method/inception-v3). Vous pouvez télécharger les poids de cette architecture obtenus par pré-entrainement sur le dataset [ImageNet](http://www.image-net.org/) (utilisé dans la classification). Ce réseau sera donc entrainé à faire de la classification d'images, mais vous allez utiliser la dernière couche cachée de ce réseau (c'est-à-dire l'avant-dernière couche car la dernière correspond aux classes de ImageNet), pour la mettre plus tard en entrée à votre RNN. Le RNN, comme nous l'avons dit précédemment, devra apprendre à faire de l'annotation en se basant sur ces représentations d'images. \n",
    "\n",
    "Comme l'entrainement s'effectue généralement en faisant plusieurs passes sur les données, l'intérêt de précalculer des représentations d'images est de ne pas avoir à faire passer à chaque fois les images dans le CNN et d'éviter les couts d'entrainement associés au CNN. L'inconvénient potentiel est que les performances seront probablement plus limitées car l'entrainement ne portera que sur les poids du RNN et pas ceux du CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZ5pbplBtvt2",
    "outputId": "83f34bfc-bc2c-46b2-d754-fa2dd6e66e3f"
   },
   "outputs": [],
   "source": [
    "# Telechargement du modèle InceptionV3 pré-entrainé avec la cassification sur ImageNet\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "# Creation d'une variable qui sera l'entrée du nouveau modèle de pre-traitement d'images\n",
    "new_input =  #A COMPLETER\n",
    "# récupérer la dernière couche caché qui contient l'image en representation compacte\n",
    "hidden_layer =  #A COMPLETER\n",
    "\n",
    "# Modèle qui calcule une representation dense des images avec InceptionV3\n",
    "image_features_extract_model =  #A COMPLETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "va0Ppu_1tvt5"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    La fonction load_image a pour entrée le chemin d'une image et pour sortie un couple\n",
    "    contenant l'image traitée ainsi que son chemin d'accès.\n",
    "    La fonction load_image effectue les traitement suivant:\n",
    "        1. Chargement du fichier correspondant au chemin d'accès image_path\n",
    "        2. Décodage de l'image en RGB.\n",
    "        3. Redimensionnement de l'image en taille (299, 299).\n",
    "        4. Normalisation des pîxels de l'image entre -1 et 1\n",
    "    \"\"\"\n",
    "    img =  #A COMPLETER\n",
    "    img =  #A COMPLETER\n",
    "    img =  #A COMPLETER\n",
    "    img =  #A COMPLETER\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehAZS9rmtvt6"
   },
   "source": [
    "Puisqu'on a déjà un CNN préentrainé, il convient mieux d'effectuer l'entrainement sur les représentations retournées par le CNN car elles sont plus compactes et donc moins couteuses en mémoire et en temps. Pour cela, il convient de les mettre en les mettre en cache au niveau du disque si votre RAM n'est pas assez grande. La dernière couche du CNN a la forme `8x8x2048`. Vous allez ensuite stocker ces images sous forme de `numpy.array` au niveau du disque grâce à la fonction `np.save()`.\n",
    "\n",
    "**Remarque:** Les traitements dans TensorFlow sont effectués par batch (c'est plus simple pour le GPU), donc il faut configurer votre batch pour pouvoir faire passer vos images dans InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnH9jynttvt8",
    "outputId": "e4671011-f46d-47aa-c635-bce3d0b773e5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Pré-traitement des images\n",
    "# Prendre les noms des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Creation d'une instance de \"tf.data.Dataset\" partant des noms des images \n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "# Division du données en batchs après application du pré-traitement fait par load_image\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour effectuez le pré-traitement d'InceptionV3\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Pré-traitement du batch (de taille (16,8,8,2048)) courant par InceptionV3 \n",
    "    batch_features =  #A COMPLETER\n",
    "    # Resize du batch de taille (16,8,8,2048) en taille (16,64,2048)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Parcourir le batch courant et stocker le chemin ainsi que le batch avec np.save()\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # (chemin de l'image associe a sa nouvelle representation , representation de l'image)\n",
    "        np.save(\n",
    "            #A COMPLETER\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_lKlRUrtvt-"
   },
   "source": [
    "# 3 Prétraitement des annotations\n",
    "\n",
    "Pour faire de l'annotation nous devons avoir la possibilité de traiter le texte de manière automatique en suivant les étapes ci-dessous :\n",
    "<ul>\n",
    "    <li>Construire un vocabulaire des mots contenus dans les légendes. Pour cela nous devons tokeniser (diviser) les mots dans les légendes sur les caractères spéciaux (espaces, ponctuation, signes spéciaux, ...).</li>\n",
    "    <li>Ce vocabulaire sera beaucoup trop volumineux, on devra se limiter par exemple aux 5000 premiers mots les plus fréquents. Vous pourrez remplacer les autres mots par un token spécial nommé \"UNK\" (unknown).</li>\n",
    "    <li>Les mots doivent aussi être numérotés dans le dictionnaire pour en former des annotations représentés de manière lisible pour les algorithme de traitement de texte.</li>\n",
    "    <li>Les exemples (annotations) seront remplis pour qu'ils aient tous la même taille (la taille maximale des annotations).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "aUdb_kXTtvuA"
   },
   "outputs": [],
   "source": [
    "# Trouver la taille maximale \n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Chosir les 5000 mots les plus frequents du vocabulaire\n",
    "top_k = 5000\n",
    "#La classe Tokenizer permet de faire du pre-traitement de texte pour reseau de neurones \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# Construit un vocabulaire en se basant sur la liste train_captions\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# Créer le token qui sert à remplir les annotations pour egaliser leurs longueur\n",
    "tokenizer.word_index['<pad>'] =  0#A COMPLETER\n",
    "tokenizer.index_word[0] =  '<pad>' #A COMPLETER\n",
    "\n",
    "# Creation des vecteurs(liste de token entiers) à partir des annotations (liste de mots)\n",
    "train_seqs =  tokenizer.texts_to_sequences(train_captions)#A COMPLETER\n",
    "\n",
    "# Remplir chaque vecteur à jusqu'à la longueur maximale des annotations\n",
    "cap_vector =  tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')#A COMPLETER\n",
    "\n",
    "# Calcule la longueur maximale qui est utilisée pour stocker les poids d'attention \n",
    "# Elle servira plus tard pour l'affichage lors de l'évaluation\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWPu43FcTWWO"
   },
   "source": [
    "Par la suite (dans le prochain workshop), nous reprendrons cette deuxième partie et nous enchaînerons avec la formation du jeu d'entrainement et de test, la construction du modèle que l'on va entraîner ainsi que la phase d'entraînement et de test."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning_vfinale.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
